{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2ee29d2",
   "metadata": {},
   "source": [
    "# 4.2 Decision Tree\n",
    "A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. The tree is constructed by recursively splitting the data into subsets based on feature\n",
    "values, forming a tree-like structure. Each internal node represents a decision based on a feature,\n",
    "and each leaf node represents a class label (for classification tasks).\n",
    "In a Decision Tree Classifier, the goal is to predict the value of a target variable by learning\n",
    "simple decision rules inferred from data features.\n",
    "## 4.2.1 How It Works\n",
    "• Root Node Creation: The tree starts at the root node and recursively splits the data using a\n",
    "feature that maximizes the \"information gain\" (for classification tasks). The splitting criterion\n",
    "could be Gini impurity or Entropy.\n",
    "\n",
    "• Recursive Partitioning: The data is split at each node until one of the following conditions\n",
    "is met:\n",
    "\n",
    "– All records belong to the same class.\n",
    "\n",
    "– The maximum depth of the tree is reached.\n",
    "\n",
    "– Other specified criteria (such as min_samples_split or min_impurity_decrease) are\n",
    "satisfied.\n",
    "\n",
    "• Tree Pruning: To prevent overfitting, the tree can be pruned by limiting its depth (max_depth)\n",
    "or setting other parameters like min_samples_split.\n",
    "\n",
    "• Classification: At the leaf nodes, the predicted class is determined based on the majority\n",
    "class of the training examples at that node.\n",
    "## 4.2.2 Advantages and Disadvantages\n",
    "Advantages:\n",
    "\n",
    "• Interpretability: Easy to visualize and interpret, especially for small trees.\n",
    "\n",
    "• Non-linearity: Can handle non-linear relationships between features.\n",
    "\n",
    "• No Data Normalization: Does not require scaling of the data.\n",
    "\n",
    "• Handling Missing Data: Can handle missing values effectively.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "• Overfitting: Decision Trees can overfit on the training data, especially when the tree is deep.\n",
    "\n",
    "• Unstable: Small changes in the data can result in a completely different tree.\n",
    "\n",
    "• Bias: Trees can be biased towards the dominant class in an imbalanced dataset\n",
    "\n",
    "## 4.2.3 Splitting Criteria\n",
    "\n",
    "• Gini Impurity:\n",
    "Gini impurity is a measure of how often a randomly chosen element would be incorrectly\n",
    "classified if randomly labeled according to the class distribution of a subset. Lower Gini\n",
    "impurity means better splits.\n",
    "Gini(S) = 1 −\n",
    "Xn\n",
    "i=1\n",
    "p\n",
    "2\n",
    "i\n",
    "where pi\n",
    "is the probability of an element being classified as class i.\n",
    "\n",
    "• Entropy (Information Gain)\n",
    "Entropy measures the amount of uncertainty in a dataset. The goal is to maximize information\n",
    "gain, which reduces the uncertainty (or entropy) after the split.\n",
    "Entropy(S) = −\n",
    "Xn\n",
    "i=1\n",
    "pi\n",
    "log2 pi\n",
    "where pi\n",
    "is the probability of an element being classified as class i (which can be estimated as\n",
    "the proportion of elements in class i).\n",
    "Information gain is calculated as:\n",
    "IG(S, A) = Entropy(S) −\n",
    "X\n",
    "v∈Values(A)\n",
    "|Sv|\n",
    "|S|\n",
    "× Entropy(Sv)\n",
    "where S is the dataset, and A is the attribute used to split S, and Sv represents the subset of\n",
    "S for which attribute A has value v.\n",
    "## 4.2.4 Parameters of Decision Tree in Scikit-learn\n",
    "Scikit-learn’s DecisionTreeClassifier offers several parameters to control tree behavior and prevent overfitting. Below is a list of key parameters:\n",
    "\n",
    "| Parameter | Description | Default Value |\n",
    "| :--- | :--- | :--- |\n",
    "| criterion | Function to measure the quality of a split. Options are \"gini\" (Gini impurity) and \"entropy\" (Information gain). | \"gini\" |\n",
    "| splitter | Strategy to split at each node. Options are \"best\" (chooses the best split) and \"random\" (chooses a random split). | \"best\" |\n",
    "| max_depth | Maximum depth of the tree. Controls tree depth to prevent overfitting. If None, nodes expand until all leaves are pure. | None |\n",
    "| min_samples_split | Minimum number of samples required to split an internal node. Increasing this helps prevent small splits and overfitting. | 2 |\n",
    "| min_samples_leaf | Minimum number of samples required to be at a leaf node. Increasing this prevents trees from having overly small leaves. | 1 |\n",
    "| min_weight_fraction_leaf | Minimum weighted fraction of the total weights required to be at a leaf node. | 0.0 |\n",
    "| max_features | Number of features to consider when looking for the best split. Can be an integer, float, \"auto\", \"sqrt\", \"log2\", or None. | None |\n",
    "| random_state | Controls the randomness of the estimator. Setting a value ensures reproducibility of results. | None |\n",
    "| max_leaf_nodes | Grow the tree with a maximum number of leaf nodes. If None, an unlimited number of leaf nodes will be allowed. | None |\n",
    "| min_impurity_decrease | Node will be split if the split induces a decrease in impurity greater than or equal to this value. | 0.0 |\n",
    "| class_weight | Weights associated with classes. If None, all classes are assumed to have weight one. | None |\n",
    "| ccp_alpha | Complexity parameter used for Minimal Cost-Complexity Pruning. The larger the value, the more the tree will be pruned. | 0.0 |\n",
    "\n",
    "## 4.2.5 Code Example of Decision Tree\n",
    "Below is a code example of a Decision Tree classifier in Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a841c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load dataset\n",
    "data=load_iris()\n",
    "X_train,X_test,y_train,y_test=train_test_split(data.data,data.target,test_size=0.2,random_state=42)\n",
    "# Train Decision Tree model\n",
    "dt_model=DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train,y_train)\n",
    "# Predict and evaluate\n",
    "y_pred=dt_model.predict(X_test)\n",
    "print(f\"Decision Tree Accuracy: {accuracy_score(y_test,y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a96409d",
   "metadata": {},
   "source": [
    "## 4.2.6 Conclusion\n",
    "The Decision Tree algorithm is a powerful classification tool, but it can easily overfit the training\n",
    "data. To mitigate overfitting, tuning hyperparameters like max_depth, min_samples_split, and\n",
    "min_samples_leaf is crucial. Using methods like Grid Search or Random Search ensures that the\n",
    "best combination of parameters is selected. These methods will be described and studied later in\n",
    "the lab (e.g., see Sec. 6).\n",
    "# 4.3 Random Forest\n",
    "Random Forest is an ensemble learning algorithm, used for both classification and regression\n",
    "tasks. It is based on the idea of building multiple decision trees and combining their predictions to\n",
    "improve model accuracy and prevent overfitting. It works by creating multiple decision trees (hence\n",
    "a \"forest\") during training time and outputting the mode of the classes (classification) or the mean\n",
    "prediction (regression) of the individual trees.\n",
    "Random Forest is built on two main ideas:\n",
    "\n",
    "• Bagging (Bootstrap Aggregation): The algorithm uses bootstrapping, i.e., creating different subsets of the training dataset (with replacement), to train individual decision trees.\n",
    "\n",
    "• Random Feature Selection: At each split in the decision tree, a random subset of the\n",
    "features is considered, instead of using all features. This helps to ensure that individual trees\n",
    "are less correlated.\n",
    "\n",
    "## 4.3.1 How It Works:\n",
    "1. Training Phase:\n",
    "\n",
    "• The algorithm creates multiple decision trees. Each tree is trained on a random subset\n",
    "of the data (sampled with replacement).\n",
    "\n",
    "• Each tree is built using a random subset of the features (selected randomly at each node\n",
    "split).\n",
    "\n",
    "• Every tree grows to the maximum possible depth (unless constrained by hyperparameters\n",
    "like max_depth or min_samples_split).\n",
    "2. Prediction Phase:\n",
    "\n",
    "• For classification tasks, each tree in the forest outputs a class label, and the final prediction\n",
    "is made based on a majority vote.\n",
    "\n",
    "• For regression tasks, the predictions of all trees are averaged to obtain the final prediction.\n",
    "4.3.2 Advantages of Random Forest:\n",
    "1. Reduction in Overfitting: Random Forest overcomes the problem of overfitting that is often\n",
    "encountered in Decision Trees. This is achieved by averaging multiple trees (in classification)\n",
    "or taking the majority vote, which lowers the variance.\n",
    "2. Robustness to Noise: Random Forest is robust to noisy data and works well even if a large\n",
    "portion of the data is missing.\n",
    "3. Feature Importance: Random Forest provides a measure of feature importance, which can\n",
    "help in feature selection.\n",
    "4. Versatility: It can handle both classification and regression problems, and can work with\n",
    "high-dimensional data.\n",
    "5. Scalability: It is highly scalable because it builds each tree independently, and therefore can\n",
    "be easily parallelized.\n",
    "4.3.3 Disadvantages of Random Forest:\n",
    "1. Complexity and Interpretability: Random Forest models are more complex and harder\n",
    "to interpret compared to individual decision trees.\n",
    "2. Computationally Intensive: Since multiple trees are grown, the algorithm can be computationally expensive and slow, especially when the number of trees is large or the dataset is\n",
    "huge.\n",
    "3. Bias-Variance Tradeoff: While Random Forest reduces variance, it can slightly increase\n",
    "bias compared to individual decision trees.\n",
    "\n",
    "## 4.3.4 Parameters of Random Forest in Scikit-learn\n",
    "The RandomForestClassifier in Scikit-learn offers several hyperparameters to control the behavior\n",
    "of the model and tune its performance:\n",
    "\n",
    "| Parameter | Description | Default Value |\n",
    "| :--- | :--- | :--- |\n",
    "| n_estimators | The number of trees in the forest. Increasing this improves performance but also increases computation time. | 100 |\n",
    "| criterion | The function to measure the quality of a split. Supported criteria are 'gini' for Gini impurity and 'entropy' for Information Gain. | 'gini' |\n",
    "| max_depth | The maximum depth of the tree. Limiting depth controls overfitting. If None, nodes are expanded until all leaves are pure. | None |\n",
    "| min_samples_split | The minimum number of samples required to split an internal node. Larger values prevent overfitting. | 2 |\n",
    "| min_samples_leaf | The minimum number of samples required to be at a leaf node. Larger values prevent deep trees with fewer samples per leaf. | 1 |\n",
    "| min_weight_fraction_leaf | The minimum weighted fraction of the input samples required to be at a leaf node. | 0.0 |\n",
    "| max_features | The number of features to consider when looking for the best split. Can be 'auto', 'sqrt', 'log2', or an integer. | 'auto' |\n",
    "| max_leaf_nodes | Grow the tree with a maximum number of leaf nodes. If None, unlimited leaf nodes are allowed. | None |\n",
    "| min_impurity_decrease | A node will be split if the split causes a decrease in impurity greater than or equal to this value. | 0.0 |\n",
    "| bootstrap | Whether bootstrap samples are used when building trees. If False, the entire dataset is used to build each tree. | True |\n",
    "| oob_score | Whether to use out-of-bag samples to estimate the generalization accuracy. | False |\n",
    "| n_jobs | The number of jobs to run in parallel. -1 means using all processors. | None |\n",
    "| random_state | Controls the randomness of the estimator. If set, ensures reproducibility of results. | None |\n",
    "| verbose | Controls the verbosity when fitting and predicting. | 0 |\n",
    "| warm_start | If True, reuse the solution of the previous call to add more estimators. | False |\n",
    "| class_weight | Weights associated with classes. If None, all classes are supposed to have weight one. | None |\n",
    "| ccp_alpha | Complexity parameter used for Minimal Cost-Complexity Pruning. The higher the value, the more the tree is pruned. | 0.0 |\n",
    "| max_samples | If bootstrap=True, the number or fraction of samples to draw from the original data to train each base estimator. | None |\n",
    "\n",
    "## 4.3.5 Code Example of Random Forest:\n",
    "Here is an example of a Random Forest Classifier using Scikit-learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99452fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load dataset\n",
    "data=load_iris()\n",
    "X_train,X_test,y_train,y_test=train_test_split(data.data,data.target,test_size=0.2,random_state=42)\n",
    "# Train Random Forest model\n",
    "rf_model=RandomForestClassifier(n_estimators=100,random_state=42)\n",
    "rf_model.fit(X_train,y_train)\n",
    "# Predict and evaluate\n",
    "y_pred=rf_model.predict(X_test)\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test,y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5843bb9a",
   "metadata": {},
   "source": [
    "## 4.3.6 Conclusion\n",
    "The Random Forest algorithm is a powerful and versatile machine learning model that can handle\n",
    "both classification and regression tasks. It reduces overfitting by averaging the results of multiple\n",
    "trees and introduces randomness into the feature selection process, which makes it more robust.\n",
    "Effective tuning of hyperparameters like n_estimators, max_depth, min_samples_split, and\n",
    "bootstrap can significantly improve performance.\n",
    "# 4.4 Support Vector Machine (SVM)\n",
    "Support Vector Machines (SVM) are a set of supervised learning algorithms primarily used for\n",
    "classification, but they can also be applied to regression and outlier detection tasks. SVMs are\n",
    "effective for both linear and non-linear data classification problems. The key idea behind SVM is to\n",
    "find a hyperplane that best separates the data points of different classes in the feature space.\n",
    "SVM aims to maximize the margin between the data points of different classes. The points that\n",
    "are closest to the hyperplane are known as support vectors, and they are the most critical elements\n",
    "of the dataset for defining the classification boundary.\n",
    "## 4.4.1 How SVM Works\n",
    "1. Linear SVM:\n",
    "• For a linearly separable dataset, SVM finds the optimal hyperplane that separates the\n",
    "data into two classes with the maximum margin. The margin is the distance between the\n",
    "hyperplane and the nearest data points from both classes (called support vectors).\n",
    "\n",
    "• Mathematically, the objective of SVM is to find a hyperplane wT x + b = 0 where:\n",
    "– w is the normal vector to the hyperplane.\n",
    "– x is the input feature vector.\n",
    "– b b is the bias term.\n",
    "\n",
    "• SVM maximizes the margin (i.e., the distance between the hyperplane and support vectors) by solving the following (constrained) optimization problem:\n",
    "min\n",
    "w,b \u0012\n",
    "1\n",
    "2\n",
    "∥w∥\n",
    "2\n",
    "\u0013\n",
    "(1)\n",
    "subject to:\n",
    "yi(w\n",
    "T xi + b) ≥ 1 ∀i (2)\n",
    "where yi are the class labels (yi ∈ {−1, 1}).\n",
    "2. Non-Linear SVM:\n",
    "\n",
    "• When data is not linearly separable, SVM uses the kernel trick (described in the next\n",
    "subsection) to map the data into a higher-dimensional space where it becomes linearly\n",
    "separable. This transformation is done implicitly without needing to compute the coordinates in the new space.\n",
    "3. Soft Margin SVM:\n",
    "\n",
    "• In practice, real-world data is often noisy or not perfectly separable. Soft margin SVM\n",
    "allows some misclassification by introducing a penalty for misclassified data points. This\n",
    "is controlled by the regularization parameter C, which balances the trade-off between\n",
    "maximizing the margin and minimizing the classification error.\n",
    "– If C is large, the model will prioritize minimizing classification errors (small margin).\n",
    "– If C is small, the model will allow more misclassifications and aim to maximize the\n",
    "margin.\n",
    "4.4.2 Kernel Functions in SVM:\n",
    "The kernel trick allows SVM to work well with non-linearly separable data by implicitly transforming\n",
    "the feature space. Some common kernels include:\n",
    "1. Linear Kernel:\n",
    "\n",
    "• No transformation, the data is assumed to be linearly separable.\n",
    "\n",
    "• Kernel function: K(x, x′\n",
    ") = x\n",
    "T x\n",
    "′\n",
    ".\n",
    "2. Polynomial Kernel:\n",
    "\n",
    "• Adds polynomial terms of features to allow non-linear classification.\n",
    "\n",
    "• Kernel function: K(x, x′\n",
    ") = (x\n",
    "T x\n",
    "′ + 1)d\n",
    ", where d is the degree of the polynomial.\n",
    "3. Radial Basis Function (RBF) Kernel:\n",
    "\n",
    "• Maps data into an infinite-dimensional space using a Gaussian function.\n",
    "\n",
    "• Kernel function: K(x, x′\n",
    ") = exp(−γ∥x − x\n",
    "′∥\n",
    "2\n",
    "), where γ defines the influence of a single\n",
    "training example.\n",
    "4. Sigmoid Kernel:\n",
    "\n",
    "• Mimics the behavior of neural networks.\n",
    "\n",
    "• Kernel function: K(x, x′\n",
    ") = tanh(αxT x\n",
    "′ + c), where α and c are kernel parameters.\n",
    "\n",
    "## 4.4.3 Advantages of SVM:\n",
    "1. Effective in High-Dimensional Spaces: SVM works well even when the number of features\n",
    "is greater than the number of samples.\n",
    "2. Robustness to Overfitting: The regularization parameter C allows control over the trade-off\n",
    "between misclassification and margin maximization, preventing overfitting.\n",
    "3. Flexibility through Kernels: Different kernel functions enable SVM to handle linear and\n",
    "non-linear classification problems.\n",
    "4. Support Vectors: Only the support vectors are used to define the decision boundary, making\n",
    "the algorithm memory efficient.\n",
    "## 4.4.4 Disadvantages of SVM:\n",
    "1. High Computational Cost: SVM can be computationally intensive, especially for large datasets.\n",
    "2. Choice of Kernel and Parameters: SVM’s performance depends heavily on the choice of kernel\n",
    "function and hyperparameters (like C andγ). Selecting the right kernel can be challenging.\n",
    "3. Sensitivity to Noise: SVM is sensitive to noisy data and outliers, especially when the data\n",
    "points are close to the decision boundary.\n",
    "## 4.4.5 Parameters of SVM in Scikit-learn:\n",
    "Scikit-learn’s SVC (Support Vector Classifier) provides several parameters that can be tuned to\n",
    "control the behavior of the model and optimize its performance.\n",
    "\n",
    "| Parameter | Description | Default Value |\n",
    "| :--- | :--- | :--- |\n",
    "| C | Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. Smaller values specify stronger regularization. | 1.0 |\n",
    "| kernel | Specifies the kernel type to be used in the algorithm. Options are: 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'. | 'rbf' |\n",
    "| degree | Degree of the polynomial kernel function (kernel='poly'). Ignored by other kernels. | 3 |\n",
    "| gamma | Kernel coefficient for 'rbf', 'poly', and 'sigmoid'. Options are: 'scale' (uses $1 / (n_{\\text{features}} \\times X.\\text{var}())$) or 'auto' (uses $1 / n_{\\text{features}}$). | 'scale' |\n",
    "| coef0 | Independent term in kernel function. It is used in 'poly' and 'sigmoid' kernels. | 0.0 |\n",
    "| shrinking | Whether to use the shrinking heuristic. | True |\n",
    "| probability | Whether to enable probability estimates. This must be set to True before calling fit. | False |\n",
    "| tol | Tolerance for stopping criterion. | 1e-3 |\n",
    "| cache_size | Size of the kernel cache (in MB). | 200 |\n",
    "| class_weight | Weights associated with classes. If not given, all classes are supposed to have weight one. 'balanced' mode uses the values inversely proportional to class frequencies. | None |\n",
    "| verbose | Controls the verbosity when fitting and predicting. | False |\n",
    "| max_iter | Hard limit on iterations within solver, or -1 for no limit. | -1 |\n",
    "| decision_function_shape | Whether to return a one-vs-rest ('ovr') decision function of shape (n_samples, n_classes) as output, or the original one-vs-one ('ovo') shape. | 'ovr' |\n",
    "| break_ties | If True, decision function will be used to break ties in multi-class classification when decision_function_shape='ovr'. | False |\n",
    "| random_state | Controls the pseudo-random number generation for shuffling the data for probability estimates. Only used when probability=True. | None |\n",
    "\n",
    "## 4.4.6 Code Example of Support Vector Machine\n",
    "Here’s an example of using the Support Vector Classifier (SVC) with Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1da80c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load dataset\n",
    "data=load_iris()\n",
    "X_train,X_test,y_train,y_test=train_test_split(data.data,data.target,test_size=0.2,random_state=42)\n",
    "# Train SVM model\n",
    "svm_model=SVC(kernel='linear',random_state=42)\n",
    "svm_model.fit(X_train,y_train)\n",
    "# Predict and evaluate\n",
    "y_pred=svm_model.predict(X_test)\n",
    "print(f\"SVM Accuracy: {accuracy_score(y_test,y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800779f4",
   "metadata": {},
   "source": [
    "# 4.5 K-Nearest Neighbors (KNN)\n",
    "The K-Nearest Neighbors (KNN) algorithm is a simple, yet powerful, non-parametric machine\n",
    "learning method used for both classification and regression tasks. It is based on the principle that\n",
    "data points that are close to each other in feature space are likely to belong to the same class (in\n",
    "classification) or have similar values (in regression).\n",
    "\n",
    "KNN is an example of a lazy learner algorithm because it does not make any assumptions\n",
    "about the data distribution, nor does it explicitly build a model during the training phase. Instead,\n",
    "it stores the training dataset and makes predictions based on the similarity between the test instance\n",
    "and the stored training instances. Accordingly, it is also an example of instance-based learning.\n",
    "\n",
    "## 4.5.1 How KNN Works\n",
    "1. Choosing k: The parameter k determines how many neighbors will be considered for the\n",
    "prediction. Typically, an odd k is chosen to avoid ties in classification problems.\n",
    "2. Distance Calculation: For each test instance, KNN calculates the distance to all the training\n",
    "points. The most common distance metric used is Euclidean distance, defined as:\n",
    "d(x, y) =\n",
    "vuutXn\n",
    "i=1\n",
    "(xi − yi)\n",
    "2\n",
    "where x and y are two points in an n-dimensional feature space.\n",
    "3. Identifying Neighbors: After computing the distances, the algorithm selects the k-nearest\n",
    "neighbors, which are the training points that have the smallest distance from the test instance.\n",
    "4. Making Predictions:\n",
    "\n",
    "• Classification: The class of the test instance is predicted based on the majority class\n",
    "among the k-nearest neighbors.\n",
    "\n",
    "• Regression: The value of the test instance is predicted as the average of the values of\n",
    "the k-nearest neighbors.\n",
    "## 4.5.2 Advantages of KNN\n",
    "\n",
    "• Simplicity: KNN is easy to implement and understand.\n",
    "\n",
    "• No Training Time: Since KNN is a lazy learner, it does not require any training time.\n",
    "\n",
    "• Non-parametric: KNN does not make any assumptions about the underlying data distribution, making it versatile and applicable to many types of data.\n",
    "\n",
    "• Flexible: KNN can be used for both classification and regression tasks.\n",
    "\n",
    "## 4.5.3 Disadvantages of KNN\n",
    "\n",
    "• Computationally Expensive: KNN needs to compute the distance between the test instance\n",
    "and all training points, which can be slow, especially for large datasets.\n",
    "\n",
    "• Storage Intensive: The algorithm must store all the training data, requiring considerable\n",
    "memory.\n",
    "\n",
    "• Sensitive to Noise and Irrelevant Features: KNN can be affected by noisy data or\n",
    "irrelevant features, as all features contribute equally to the distance calculation.\n",
    "\n",
    "• Choice of k: Choosing an optimal value for k is crucial. A small k may lead to overfitting,\n",
    "while a large k may lead to underfitting.\n",
    "\n",
    "## 4.5.4 Parameters of KNN in Scikit-learn\n",
    "The KNeighborsClassifier in Scikit-learn offers several parameters to control the behavior of the\n",
    "KNN algorithm:\n",
    "\n",
    "\n",
    "| Parameter | Description | Default Value |\n",
    "| :--- | :--- | :--- |\n",
    "| n_neighbors | The number of neighbors to use for k-nearest neighbors voting. | 5 |\n",
    "| weights | Weight function used in prediction. Can be: 'uniform' (all points weighted equally), 'distance' (closer points have higher influence), or a custom function. | 'uniform' |\n",
    "| algorithm | Algorithm used to compute the nearest neighbors. Options are: 'auto' (chooses best algorithm), 'ball_tree' (BallTree), 'kd_tree' (KDTree), 'brute' (Brute-force). | 'auto' |\n",
    "| leaf_size | Leaf size passed to BallTree or KDTree algorithms. It affects the speed of construction/query and memory usage. | 30 |\n",
    "| p | The power parameter for the Minkowski distance: p = 1 (Manhattan), p = 2 (Euclidean). | 2 |\n",
    "| metric | The distance metric to use. Default is Minkowski: $d(x, y) = \\left(\\sum|x_i - y_i|^p\\right)^{1/p}$ | 'minkowski' |\n",
    "| metric_params | Additional keyword arguments for the metric function. | None |\n",
    "| n_jobs | The number of parallel jobs to run for neighbors search. If -1, all processors are used. | None |\n",
    "\n",
    "## 4.5.5 Code Example\n",
    "Here’s an example of using the K-Nearest Neighbors algorithm in Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbbd8c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load dataset\n",
    "data=load_iris()\n",
    "X_train,X_test,y_train,y_test=train_test_split(data.data,data.target,test_size=0.2,random_state=42)\n",
    "# Train KNN model\n",
    "knn_model=KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train,y_train)\n",
    "# Predict and evaluate\n",
    "y_pred=knn_model.predict(X_test)\n",
    "print(f\"KNN Accuracy: {accuracy_score(y_test,y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52182655",
   "metadata": {},
   "source": [
    "## 4.5.6 Conclusion\n",
    "The K-Nearest Neighbors (KNN) algorithm is a simple and effective tool for both classification and\n",
    "regression tasks. It relies on the intuition that similar points are likely to belong to the same class\n",
    "or have similar values. While it is easy to understand and implement, KNN is computationally\n",
    "expensive and sensitive to the choice of hyperparameters like k and the distance metric. Tuning\n",
    "these parameters can significantly impact the performance of the model.\n",
    "\n",
    "# 4.6 Logistic Regression (LR)\n",
    "Logistic Regression is a supervised learning algorithm used for binary classification problems,\n",
    "where the output variable is categorical and consists of two classes (0 or 1, true or false, etc.).\n",
    "Despite its name, logistic regression is not a regression algorithm but a classification one. The\n",
    "algorithm estimates the probability that a given instance belongs to a certain class by fitting data\n",
    "to a logistic (sigmoid) function.\n",
    "## 4.6.1 Sigmoid Function\n",
    "Logistic regression models the probability of a binary outcome using a sigmoid function, which is\n",
    "defined as:\n",
    "σ(z) = 1\n",
    "1 + e−z\n",
    "where z = w\n",
    "T x + b is the linear combination of the input features x, weights w, and bias b. The\n",
    "output of the sigmoid function is a value between 0 and 1, which can be interpreted as the probability\n",
    "of the instance belonging to the positive class.\n",
    "P(y = 1|x) = 1\n",
    "1 + e−(wT x+b)\n",
    "## 4.6.2 How Logistic Regression Works\n",
    "1. Linear Model: Logistic regression begins by fitting a linear model w\n",
    "T x+b, where w represents\n",
    "the coefficients (weights) for the features and b represents the bias\n",
    "2. Sigmoid Transformation: The linear model’s output is passed through the sigmoid function\n",
    "to ensure the output lies between 0 and 1, representing the probability of the sample belonging\n",
    "to the positive class.\n",
    "3. Thresholding: Once the probability is estimated, a threshold (usually 0.5) is applied to\n",
    "classify the instance into one of two classes:\n",
    "yˆ =\n",
    "(\n",
    "1 if P(y = 1|x) ≥ 0.5\n",
    "0 if P(y = 1|x) < 0.5\n",
    "4. Cost Function: The algorithm uses the logistic loss (also known as log-loss or binary crossentropy) as the cost function:\n",
    "J(w, b) = −\n",
    "1\n",
    "m\n",
    "Xm\n",
    "i=1\n",
    "h\n",
    "y\n",
    "(i)\n",
    "log(P(y = 1|x\n",
    "(i)\n",
    ")) + (1 − y\n",
    "(i)\n",
    ") log(1 − P(y = 1|x\n",
    "(i)\n",
    "))i\n",
    "5. Optimization: The weights w and bias b are optimized by minimizing the cost function using\n",
    "techniques like gradient descent.\n",
    "## 4.6.3 Advantages of Logistic Regression\n",
    "\n",
    "• Interpretability: The model’s coefficients can be interpreted as the impact of each feature\n",
    "on the predicted probability.\n",
    "\n",
    "• Probability Output: Logistic regression provides probability estimates, which can be useful\n",
    "for ranking or prioritizing instances.\n",
    "\n",
    "• Computationally Efficient: It is efficient for small to medium-sized datasets and converges\n",
    "quickly.\n",
    "\n",
    "• No Feature Scaling Required (for some variants): In basic logistic regression, feature\n",
    "scaling is not mandatory.\n",
    "## 4.6.4 Disadvantages of Logistic Regression\n",
    "\n",
    "• Linear Decision Boundary: Logistic regression assumes a linear relationship between features and the log-odds of the outcome, which may not be appropriate for non-linear data.\n",
    "\n",
    "• Imbalanced Data: Logistic regression may struggle with imbalanced datasets, as it tends to\n",
    "predict the majority class.\n",
    "\n",
    "• Overfitting with High-Dimensional Data: When there are too many features, logistic\n",
    "regression can easily overfit the training data. Regularization techniques like L1 or L2 are\n",
    "often needed to address this.\n",
    "## 4.6.5 Parameters of Logistic Regression in Scikit-learn\n",
    "The LogisticRegression class in Scikit-learn provides several parameters that allow the user to\n",
    "customize the behavior of the logistic regression model. Below is a list of these parameters:\n",
    "\n",
    "| Parameter | Description | Default Value |\n",
    "| :--- | :--- | :--- |\n",
    "| penalty | Specifies the regularization type. Options: 'l1', 'l2' (Ridge), 'elasticnet', 'none'. | 'l2' |\n",
    "| dual | Whether to solve the dual optimization problem (only for penalty='l2' and solver='liblinear'). | False |\n",
    "| tol | Tolerance for stopping criteria during optimization. | 1e-4 |\n",
    "| C | Inverse of the regularization strength. Smaller values specify stronger regularization. | 1.0 |\n",
    "| fit_intercept | Whether to include an intercept (bias term) in the model. | True |\n",
    "| intercept_scaling | Scaling factor for the intercept when using solver='liblinear'. | 1 |\n",
    "| class_weight | Weights associated with each class. Can be None (equal weights) or 'balanced' (inversely proportional to class frequencies). | None |\n",
    "| solver | Algorithm for optimization. Options: 'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'. | 'lbfgs' |\n",
    "| max_iter | Maximum number of iterations for the solver to converge. | 100 |\n",
    "| multi_class | Strategy for handling multiple classes: 'auto' (chooses 'ovr' or 'multinomial'), 'ovr' (One-vs-rest), 'multinomial'. | 'auto' |\n",
    "| verbose | Enables verbose output for the solver. | 0 |\n",
    "| warm_start | Whether to reuse the previous solution for the optimization. | False |\n",
    "| n_jobs | The number of parallel jobs to run for solver='sag' and 'saga'. | None |\n",
    "\n",
    "## 4.6.6 Code Example of Logistic Regression\n",
    "Here’s an example of using the Logistic Regression algorithm in Scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "890658a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load dataset\n",
    "iris=load_iris()\n",
    "X_train,X_test,y_train,y_test=train_test_split(iris.data,iris.target,test_size=0.2,random_state=42)\n",
    "# Initialize Logistic Regression\n",
    "log_reg=LogisticRegression()\n",
    "# Train the model\n",
    "log_reg.fit(X_train,y_train)\n",
    "# Predict on the test set\n",
    "y_pred=log_reg.predict(X_test)\n",
    "# Evaluate the model\n",
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00157c9",
   "metadata": {},
   "source": [
    "## 4.6.7 Conclusion\n",
    "Logistic Regression is a simple and widely used classification algorithm that models the probability\n",
    "of a binary outcome. It works by applying a logistic transformation to a linear model, producing\n",
    "outputs between 0 and 1. Logistic regression is effective for many problems, but its performance\n",
    "depends on the underlying data and the choice of regularization techniques.\n",
    "# 5 Performance Evaluation Metrics\n",
    "In machine learning, evaluating model performance is essential to understand how well the model\n",
    "generalizes to unseen data. Below are some common performance evaluation metrics used for classification and regression tasks.\n",
    "# 5.1 Accuracy\n",
    "Accuracy is the ratio of correctly predicted observations to the total observations. It is the most\n",
    "intuitive performance measure and is used when the data is balanced.\n",
    "Accuracy = \n",
    "TP + TN\n",
    "TP + TN + FP + FN\n",
    "where:\n",
    "\n",
    "• TP: True Positives\n",
    "\n",
    "• TN: True Negatives\n",
    "\n",
    "• FP: False Positives\n",
    "\n",
    "• FN: False Negatives\n",
    "# 5.2 Precision\n",
    "Precision (also called positive predictive value) is the ratio of correctly predicted positive observations to the total predicted positive observations. Precision is used when the cost of false positives\n",
    "is high.\n",
    "Precision =\n",
    "TP\n",
    "TP + FP\n",
    "5.3 Recall (Sensitivity or True Positive Rate)\n",
    "Recall (also called sensitivity or true positive rate) is the ratio of correctly predicted positive\n",
    "observations to all observations in the actual class. Recall is useful when the cost of false negatives\n",
    "is high.\n",
    "Recall =\n",
    "TP\n",
    "TP + FN\n",
    "# 5.4 F1-Score\n",
    "The F1-Score is the harmonic mean of precision and recall. It is a useful metric when you need a\n",
    "balance between precision and recall.\n",
    "F1-Score = 2 ×\n",
    "Precision × Recall\n",
    "Precision + Recall\n",
    "# 5.5 Specificity (True Negative Rate)\n",
    "Specificity is the ratio of correctly predicted negative observations to all actual negatives. It is\n",
    "useful for evaluating the performance of a model on negative observations.\n",
    "Specificity =\n",
    "TN\n",
    "TN + FP\n",
    "# 5.6 ROC-AUC (Receiver Operating Characteristic - Area Under Curve)\n",
    "The ROC curve is a graphical representation of the performance of a classification model at different\n",
    "threshold values. The AUC (Area Under Curve) summarizes the ROC curve into a single value,\n",
    "with 1 representing a perfect classifier and 0.5 representing random guessing.\n",
    "The ROC curve is plotted with:\n",
    "• True Positive Rate (TPR) on the y-axis, given by: TP\n",
    "TP+FN\n",
    "• False Positive Rate (FPR) on the x-axis, given by: FP\n",
    "FP+TN\n",
    "The AUC is calculated as the area under this curve.\n",
    "# 5.7 Logarithmic Loss (Log Loss)\n",
    "Log Loss measures the performance of a classification model where the output is a probability value\n",
    "between 0 and 1. Lower log loss values indicate better model performance.\n",
    "For binary classification, log loss is defined as:\n",
    "Log Loss = −\n",
    "1\n",
    "N\n",
    "X\n",
    "N\n",
    "i=1\n",
    "[yi\n",
    "log(pi) + (1 − yi) log(1 − pi)]\n",
    "where:\n",
    "\n",
    "• N is the number of samples.\n",
    "\n",
    "• yi\n",
    "is the actual label (0 or 1).\n",
    "\n",
    "• pi\n",
    "is the predicted probability for class 1.\n",
    "# 5.8 Mean Absolute Error (MAE)\n",
    "For regression tasks, Mean Absolute Error (MAE) is the average of the absolute differences\n",
    "between predicted and actual values.\n",
    "MAE =\n",
    "1\n",
    "n\n",
    "Xn\n",
    "i=1\n",
    "|yi − yˆi\n",
    "|\n",
    "where:\n",
    "\n",
    "• yi\n",
    "is the actual value.\n",
    "\n",
    "• yˆi\n",
    "is the predicted value.\n",
    "\n",
    "• n is the number of samples.\n",
    "# 5.9 Mean Squared Error (MSE)\n",
    "The Mean Squared Error (MSE) is the average of the squared differences between predicted and\n",
    "actual values.\n",
    "MSE =\n",
    "1\n",
    "n\n",
    "Xn\n",
    "i=1\n",
    "(yi − yˆi)\n",
    "2\n",
    "# 5.10 Root Mean Squared Error (RMSE)\n",
    "The Root Mean Squared Error (RMSE) is the square root of the mean squared error. It is\n",
    "commonly used because it has the same units as the output variable.\n",
    "RMSE =\n",
    "vuut\n",
    "1\n",
    "n\n",
    "Xn\n",
    "i=1\n",
    "(yi − yˆi)\n",
    "2\n",
    "# 5.11 R-squared (Coefficient of Determination)\n",
    "The R-squared metric measures how well the regression predictions fit the actual data. It is\n",
    "the proportion of the variance in the dependent variable that is predictable from the independent\n",
    "variables.\n",
    "R\n",
    "2 = 1 −\n",
    "Pn\n",
    "i=1(yi − yˆi)\n",
    "2\n",
    "Pn\n",
    "i=1(yi − y¯)\n",
    "2\n",
    "where y¯ is the mean of the actual values.\n",
    "\n",
    "# 6 Hyperparameter Tuning and Optimization Techniques\n",
    "Before implementing the optimization techniques, it is important to understand their practical operating properties and performance characteristics.\n",
    "# 6.1 Overview and General Guidelines\n",
    "Hyperparameter Tuning is the process of selecting the optimal hyperparameters that govern the\n",
    "training process of a machine learning model. Unlike model parameters learned during training,\n",
    "hyperparameters are set prior to the training process.\n",
    "Various optimization techniques can be considered for hyperparameter tuning, including the\n",
    "following popular approaches.\n",
    "1. Grid Search:\n",
    "• Description: An exhaustive search method where all possible combinations of hyperparameters are tried.\n",
    "• Pros: Simple and guarantees finding the best combination within the specified grid.\n",
    "• Cons: Computationally intensive, especially with a large number of hyperparameters or\n",
    "extensive ranges.\n",
    "2. Random Search:\n",
    "• Description: Randomly selects combinations of hyperparameters to try.\n",
    "• Pros: More efficient than grid search for high-dimensional spaces; can find good hyperparameters with fewer iterations.\n",
    "• Cons: May miss the optimal combination due to randomness.\n",
    "3. Bayesian Optimization:\n",
    "• Description: Builds a probabilistic model of the objective function and uses it to select\n",
    "the most promising hyperparameters to evaluate next.\n",
    "• Pros: Efficient and can find better hyperparameters with fewer evaluations.\n",
    "• Cons: More complex and computational overhead due to model fitting.\n",
    "4. Genetic Algorithms:\n",
    "• Description: Mimics the process of natural selection by creating a population of solutions\n",
    "and evolving them over generations.\n",
    "• Pros: Good at exploring large, complex search spaces and avoiding local minima.\n",
    "• Cons: Computationally expensive and requires careful parameter settings.\n",
    "For this lab, we will investigate the first three approaches described above, with code templates\n",
    "provided in the subsequent subsections.\n",
    "# 6.2 Grid SearchCV\n",
    "The following is a code template for implementing Grid Search to find the best hyperparameters for\n",
    "the Random Forest model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c751708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Best cross-validation accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load dataset\n",
    "data=load_iris()\n",
    "X_train,X_test,y_train,y_test=train_test_split(data.data,data.target,test_size=0.2,random_state=42)\n",
    "# Hyperparameter grid\n",
    "param_grid={'n_estimators':[50,100,150],'max_depth':[None,10,20],'min_samples_split':[2,5,10]}\n",
    "# Grid Search\n",
    "grid_search=GridSearchCV(RandomForestClassifier(random_state=42),param_grid,cv=3,scoring='accuracy',n_jobs=-1)\n",
    "grid_search.fit(X_train,y_train)\n",
    "# Best parameters and accuracy\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e84d8cf",
   "metadata": {},
   "source": [
    "# 6.3 RandomizedSearchCV\n",
    "The following is a code template to use Random Search for hyperparameter tuning on the Random\n",
    "Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ab9ce2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': np.int64(150), 'min_samples_split': np.int64(6), 'max_depth': np.int64(15)}\n",
      "Best cross-validation accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load dataset\n",
    "data=load_iris()\n",
    "X_train,X_test,y_train,y_test=train_test_split(data.data,data.target,test_size=0.2,random_state=42)\n",
    "# Hyperparameter distribution\n",
    "param_dist={'n_estimators':np.linspace(50,200,num=10,dtype=int),'max_depth':[None]+list(np.arange(5,25,5)),'min_samples_split':np.arange(2,11)}\n",
    "# Random Search\n",
    "random_search=RandomizedSearchCV(RandomForestClassifier(random_state=42),param_distributions=param_dist,n_iter=20,cv=3,scoring='accuracy',random_state=42,n_jobs=-1)\n",
    "random_search.fit(X_train,y_train)\n",
    "# Best parameters and accuracy\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {random_search.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc0081",
   "metadata": {},
   "source": [
    "# 6.4 Bayesian Optimization with Optuna\n",
    "The following is a code template to implement Bayesian Optimization using the Optuna library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67de4414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 23:31:30,737] A new study created in memory with name: no-name-e8a63096-31c2-4f61-ac13-1dc3aac946c3\n",
      "[I 2025-10-30 23:31:30,927] Trial 0 finished with value: 0.9500000000000001 and parameters: {'n_estimators': 136, 'max_depth': 27, 'min_samples_split': 6}. Best is trial 0 with value: 0.9500000000000001.\n",
      "[I 2025-10-30 23:31:31,151] Trial 1 finished with value: 0.9500000000000001 and parameters: {'n_estimators': 172, 'max_depth': 6, 'min_samples_split': 9}. Best is trial 0 with value: 0.9500000000000001.\n",
      "[I 2025-10-30 23:31:31,449] Trial 2 finished with value: 0.9500000000000001 and parameters: {'n_estimators': 193, 'max_depth': 16, 'min_samples_split': 3}. Best is trial 0 with value: 0.9500000000000001.\n",
      "[I 2025-10-30 23:31:31,738] Trial 3 finished with value: 0.9500000000000001 and parameters: {'n_estimators': 196, 'max_depth': 29, 'min_samples_split': 6}. Best is trial 0 with value: 0.9500000000000001.\n",
      "[I 2025-10-30 23:31:31,876] Trial 4 finished with value: 0.9583333333333334 and parameters: {'n_estimators': 86, 'max_depth': 10, 'min_samples_split': 2}. Best is trial 4 with value: 0.9583333333333334.\n",
      "[I 2025-10-30 23:31:32,015] Trial 5 finished with value: 0.9500000000000001 and parameters: {'n_estimators': 67, 'max_depth': 25, 'min_samples_split': 5}. Best is trial 4 with value: 0.9583333333333334.\n",
      "[I 2025-10-30 23:31:32,143] Trial 6 finished with value: 0.9500000000000001 and parameters: {'n_estimators': 64, 'max_depth': 28, 'min_samples_split': 5}. Best is trial 4 with value: 0.9583333333333334.\n",
      "[I 2025-10-30 23:31:32,324] Trial 7 finished with value: 0.9500000000000001 and parameters: {'n_estimators': 101, 'max_depth': 7, 'min_samples_split': 8}. Best is trial 4 with value: 0.9583333333333334.\n",
      "[I 2025-10-30 23:31:32,591] Trial 8 finished with value: 0.9500000000000001 and parameters: {'n_estimators': 139, 'max_depth': 16, 'min_samples_split': 5}. Best is trial 4 with value: 0.9583333333333334.\n",
      "[I 2025-10-30 23:31:32,716] Trial 9 finished with value: 0.9500000000000001 and parameters: {'n_estimators': 71, 'max_depth': 11, 'min_samples_split': 8}. Best is trial 4 with value: 0.9583333333333334.\n",
      "[I 2025-10-30 23:31:32,878] Trial 10 finished with value: 0.9500000000000001 and parameters: {'n_estimators': 102, 'max_depth': 21, 'min_samples_split': 2}. Best is trial 4 with value: 0.9583333333333334.\n",
      "[I 2025-10-30 23:31:33,220] Trial 11 finished with value: 0.9500000000000001 and parameters: {'n_estimators': 137, 'max_depth': 12, 'min_samples_split': 3}. Best is trial 4 with value: 0.9583333333333334.\n",
      "[I 2025-10-30 23:31:33,423] Trial 12 finished with value: 0.9500000000000001 and parameters: {'n_estimators': 106, 'max_depth': 21, 'min_samples_split': 10}. Best is trial 4 with value: 0.9583333333333334.\n",
      "[I 2025-10-30 23:31:33,714] Trial 13 finished with value: 0.9583333333333334 and parameters: {'n_estimators': 158, 'max_depth': 11, 'min_samples_split': 2}. Best is trial 4 with value: 0.9583333333333334.\n",
      "[I 2025-10-30 23:31:34,055] Trial 14 finished with value: 0.9583333333333334 and parameters: {'n_estimators': 163, 'max_depth': 11, 'min_samples_split': 2}. Best is trial 4 with value: 0.9583333333333334.\n",
      "[I 2025-10-30 23:31:34,199] Trial 15 finished with value: 0.9500000000000001 and parameters: {'n_estimators': 86, 'max_depth': 9, 'min_samples_split': 3}. Best is trial 4 with value: 0.9583333333333334.\n",
      "[I 2025-10-30 23:31:34,437] Trial 16 finished with value: 0.9500000000000001 and parameters: {'n_estimators': 163, 'max_depth': 14, 'min_samples_split': 4}. Best is trial 4 with value: 0.9583333333333334.\n",
      "[I 2025-10-30 23:31:34,617] Trial 17 finished with value: 0.9583333333333334 and parameters: {'n_estimators': 116, 'max_depth': 19, 'min_samples_split': 2}. Best is trial 4 with value: 0.9583333333333334.\n",
      "[I 2025-10-30 23:31:34,717] Trial 18 finished with value: 0.9583333333333334 and parameters: {'n_estimators': 51, 'max_depth': 5, 'min_samples_split': 4}. Best is trial 4 with value: 0.9583333333333334.\n",
      "[I 2025-10-30 23:31:34,874] Trial 19 finished with value: 0.9500000000000001 and parameters: {'n_estimators': 87, 'max_depth': 13, 'min_samples_split': 4}. Best is trial 4 with value: 0.9583333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 86, 'max_depth': 10, 'min_samples_split': 2}\n",
      "Best cross-validation accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load dataset\n",
    "data=load_iris()\n",
    "X_train,X_test,y_train,y_test=train_test_split(data.data,data.target,test_size=0.2,random_state=42)\n",
    "# Objective function\n",
    "def objective(trial):\n",
    "    n_estimators=trial.suggest_int('n_estimators',50,200)\n",
    "    max_depth=trial.suggest_int('max_depth',5,30)\n",
    "    min_samples_split=trial.suggest_int('min_samples_split',2,10)\n",
    "    rf=RandomForestClassifier(n_estimators=n_estimators,max_depth=max_depth,min_samples_split=min_samples_split,random_state=42)\n",
    "    score=cross_val_score(rf,X_train,y_train,cv=3,scoring='accuracy',n_jobs=-1)\n",
    "    return score.mean()\n",
    "# Study\n",
    "study=optuna.create_study(direction='maximize')\n",
    "study.optimize(objective,n_trials=20)\n",
    "# Best parameters and accuracy\n",
    "print(f\"Best parameters: {study.best_params}\")\n",
    "print(f\"Best cross-validation accuracy: {study.best_value:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asn1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
