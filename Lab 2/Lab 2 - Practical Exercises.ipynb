{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7a86fae-418b-4129-89d1-8b85aa4abf38",
   "metadata": {},
   "source": [
    "# 4.4 Practical Exercises: First set\n",
    "For this first set of exercises, the relevant dataset is available via the following link: Click here to see and download the dataset\n",
    "\n",
    "• Collect the relevant dataset from the given Dataset folder, and handle the missing values in this dataset.\n",
    "\n",
    "• Are there any outliers in the dataset? If yes, remove the outliers.\n",
    "\n",
    "• Normalize the appropriate features and show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3455f4d6-308d-4f88-bc1d-9f455ebc1657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Head after Outlier Capping and Min-Max Normalization:\n",
      "   Number of sexual partners  First sexual intercourse  Num of pregnancies  \\\n",
      "0                   0.857143                  0.375000            0.166667   \n",
      "1                   0.000000                  0.291667            0.166667   \n",
      "2                   0.000000                       NaN            0.166667   \n",
      "3                   1.000000                  0.458333            0.666667   \n",
      "4                   0.571429                  0.875000            0.666667   \n",
      "\n",
      "   Smokes (years)  Smokes (packs/year)  Hormonal Contraceptives (years)  \\\n",
      "0             0.0                  0.0                              0.0   \n",
      "1             0.0                  0.0                              0.0   \n",
      "2             0.0                  0.0                              0.0   \n",
      "3             0.0                  0.0                              0.4   \n",
      "4             0.0                  0.0                              1.0   \n",
      "\n",
      "   IUD (years)  STDs (number)  STDs: Time since first diagnosis  \\\n",
      "0          0.0            0.0                               NaN   \n",
      "1          0.0            0.0                               NaN   \n",
      "2          0.0            0.0                               NaN   \n",
      "3          0.0            0.0                               NaN   \n",
      "4          0.0            0.0                               NaN   \n",
      "\n",
      "   STDs: Time since last diagnosis  \n",
      "0                              NaN  \n",
      "1                              NaN  \n",
      "2                              NaN  \n",
      "3                              NaN  \n",
      "4                              NaN  \n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "file_path = \"cervical-cancer_csv.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.replace('?', np.nan)\n",
    "\n",
    "year_columns = [\n",
    "    'Smokes (years)',\n",
    "    'Hormonal Contraceptives (years)',\n",
    "    'IUD (years)',\n",
    "    'STDs: Time since first diagnosis',\n",
    "    'STDs: Time since last diagnosis'\n",
    "]\n",
    "\n",
    "features_to_process = [\n",
    "    'Number of sexual partners', 'First sexual intercourse', 'Num of pregnancies',\n",
    "    'Smokes (years)', 'Smokes (packs/year)', 'Hormonal Contraceptives (years)',\n",
    "    'IUD (years)', 'STDs (number)', 'STDs: Time since first diagnosis',\n",
    "    'STDs: Time since last diagnosis'\n",
    "]\n",
    "\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "for col in features_to_process:\n",
    "    s = df_cleaned[col].dropna() # Use non-NaN values for calculation\n",
    "    \n",
    "    if not s.empty:\n",
    "        Q1 = s.quantile(0.25)\n",
    "        Q3 = s.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Calculate bounds\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Cap the outliers (Winsorization)\n",
    "        # Values below the lower bound are set to the lower bound\n",
    "        df_cleaned[col] = np.where(df_cleaned[col] < lower_bound, lower_bound, df_cleaned[col])\n",
    "        # Values above the upper bound are set to the upper bound\n",
    "        df_cleaned[col] = np.where(df_cleaned[col] > upper_bound, upper_bound, df_cleaned[col])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# for col in features_to_process:\n",
    "    # Scale only the non-NaN values (NaNs will remain NaN)\n",
    "    # non_nan_mask = df_cleaned[col].notna()\n",
    "    # data_to_scale = df_cleaned[col][non_nan_mask].values.reshape(-1, 1)\n",
    "    \n",
    "    # # Apply the scaler and place the scaled values back\n",
    "    # df_cleaned.loc[non_nan_mask, col] = scaler.fit_transform(data_to_scale).flatten()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_cleaned), columns=df_cleaned.columns)\n",
    "\n",
    "# df_cleaned[year_columns] = df_cleaned[year_columns].apply(np.floor).astype('Int64')\n",
    "\n",
    "print(\"\\nDataFrame Head after Outlier Capping and Min-Max Normalization:\")\n",
    "print(df_scaled[features_to_process].head())\n",
    "\n",
    "df_scaled.to_csv('cervical-cancer_cleaned_normalized.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68dff17-69f3-4ecf-9002-5aaddc3c8aa0",
   "metadata": {},
   "source": [
    "# 5.6 Practical Exercises: Second set\n",
    "For this second set of exercises, the relevant dataset is available via the following link: Click here to see and download the dataset\n",
    "\n",
    "1. For the dataset, check the feature type and convert them into appropriate format.\n",
    "2. Apply suitable encoding techniques to relevant features, making them suitable for machine\n",
    "learning algorithms.\n",
    "3. Show the feature interaction and polynomial features for the dataset.\n",
    "4. Select the important features from the selected dataset.\n",
    "5. Reduce the dimensionality using the PCA technique. Observe and comment on your resutls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fea5db2-0af5-4232-8139-799d59f65ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 1. Data Types after Conversion ###\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 25 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   age             391 non-null    float64\n",
      " 1   bp              388 non-null    float64\n",
      " 2   sg              353 non-null    float64\n",
      " 3   al              354 non-null    float64\n",
      " 4   su              351 non-null    float64\n",
      " 5   rbc             248 non-null    object \n",
      " 6   pc              335 non-null    object \n",
      " 7   pcc             396 non-null    float64\n",
      " 8   ba              396 non-null    float64\n",
      " 9   bgr             356 non-null    float64\n",
      " 10  bu              381 non-null    float64\n",
      " 11  sc              383 non-null    float64\n",
      " 12  sod             313 non-null    float64\n",
      " 13  pot             312 non-null    float64\n",
      " 14  hemo            348 non-null    float64\n",
      " 15  pcv             329 non-null    float64\n",
      " 16  wc              294 non-null    float64\n",
      " 17  rc              269 non-null    float64\n",
      " 18  htn             398 non-null    float64\n",
      " 19  dm              392 non-null    float64\n",
      " 20  cad             396 non-null    float64\n",
      " 21  appet           399 non-null    float64\n",
      " 22  pe              399 non-null    float64\n",
      " 23  ane             399 non-null    float64\n",
      " 24  classification  398 non-null    float64\n",
      "dtypes: float64(23), object(2)\n",
      "memory usage: 78.3+ KB\n",
      "None\n",
      "\n",
      "==================================================\n",
      "\n",
      "### Data Head after Imputation (Mean/Mode) ###\n",
      "    age    bp     sg   al   su     rbc        pc  pcc   ba         bgr  ...  \\\n",
      "0  48.0  80.0  1.020  1.0  0.0  normal    normal  0.0  0.0  121.000000  ...   \n",
      "1   7.0  50.0  1.020  4.0  0.0  normal    normal  0.0  0.0  148.036517  ...   \n",
      "2  62.0  80.0  1.010  2.0  3.0  normal    normal  0.0  0.0  423.000000  ...   \n",
      "3  48.0  70.0  1.005  4.0  0.0  normal  abnormal  1.0  0.0  117.000000  ...   \n",
      "4  51.0  80.0  1.010  2.0  0.0  normal    normal  0.0  0.0  106.000000  ...   \n",
      "\n",
      "    pcv      wc        rc  htn   dm  cad  appet   pe  ane  classification  \n",
      "0  44.0  7800.0  5.200000  1.0  1.0  0.0    1.0  0.0  0.0             1.0  \n",
      "1  38.0  6000.0  4.707435  0.0  0.0  0.0    1.0  0.0  0.0             1.0  \n",
      "2  31.0  7500.0  4.707435  0.0  1.0  0.0    0.0  0.0  1.0             1.0  \n",
      "3  32.0  6700.0  3.900000  1.0  0.0  0.0    0.0  1.0  1.0             1.0  \n",
      "4  35.0  7300.0  4.600000  0.0  0.0  0.0    1.0  0.0  0.0             1.0  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Encoding for 'rbc': {'abnormal': 0, 'normal': 1}\n",
      "Encoding for 'pc': {'abnormal': 0, 'normal': 1}\n",
      "\n",
      "### 2. Data Head after Label Encoding 'rbc' and 'pc' ###\n",
      "    age    bp     sg   al   su  rbc  pc  pcc   ba         bgr  ...   pcv  \\\n",
      "0  48.0  80.0  1.020  1.0  0.0    1   1  0.0  0.0  121.000000  ...  44.0   \n",
      "1   7.0  50.0  1.020  4.0  0.0    1   1  0.0  0.0  148.036517  ...  38.0   \n",
      "2  62.0  80.0  1.010  2.0  3.0    1   1  0.0  0.0  423.000000  ...  31.0   \n",
      "3  48.0  70.0  1.005  4.0  0.0    1   0  1.0  0.0  117.000000  ...  32.0   \n",
      "4  51.0  80.0  1.010  2.0  0.0    1   1  0.0  0.0  106.000000  ...  35.0   \n",
      "\n",
      "       wc        rc  htn   dm  cad  appet   pe  ane  classification  \n",
      "0  7800.0  5.200000  1.0  1.0  0.0    1.0  0.0  0.0             1.0  \n",
      "1  6000.0  4.707435  0.0  0.0  0.0    1.0  0.0  0.0             1.0  \n",
      "2  7500.0  4.707435  0.0  1.0  0.0    0.0  0.0  1.0             1.0  \n",
      "3  6700.0  3.900000  1.0  0.0  0.0    0.0  1.0  1.0             1.0  \n",
      "4  7300.0  4.600000  0.0  0.0  0.0    1.0  0.0  0.0             1.0  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "\n",
      "==================================================\n",
      "\n",
      "### 3. Polynomial Features (Degree 2) ###\n",
      "Original features: 24\n",
      "New total features (with interactions/polynomials): 324\n",
      "First 5 rows and 5 columns of the new polynomial features:\n",
      "        age        bp        sg        al        su\n",
      "0 -0.205464  0.262338  0.483355 -0.013338 -0.437797\n",
      "1 -2.623810 -1.966580  0.483355  2.347516 -0.437797\n",
      "2  0.620313  0.262338 -1.381391  0.773613  2.479925\n",
      "3 -0.205464 -0.480635 -2.313764  2.347516 -0.437797\n",
      "4 -0.028511  0.262338 -1.381391  0.773613 -0.437797\n",
      "\n",
      "==================================================\n",
      "\n",
      "### 4. Selected Top 20 Features ###\n",
      "Selected feature names:\n",
      "['bp', 'sg', 'al', 'su', 'rbc', 'pc', 'pcc', 'bgr', 'bu', 'sc', 'sod', 'hemo', 'pcv', 'rc', 'htn', 'dm', 'cad', 'appet', 'pe', 'ane']\n",
      "\n",
      "Head of selected feature data:\n",
      "     bp     sg   al   su  rbc   pc  pcc         bgr    bu   sc         sod  \\\n",
      "0  80.0  1.020  1.0  0.0  1.0  1.0  0.0  121.000000  36.0  1.2  137.528754   \n",
      "1  50.0  1.020  4.0  0.0  1.0  1.0  0.0  148.036517  18.0  0.8  137.528754   \n",
      "2  80.0  1.010  2.0  3.0  1.0  1.0  0.0  423.000000  53.0  1.8  137.528754   \n",
      "3  70.0  1.005  4.0  0.0  1.0  0.0  1.0  117.000000  56.0  3.8  111.000000   \n",
      "4  80.0  1.010  2.0  0.0  1.0  1.0  0.0  106.000000  26.0  1.4  137.528754   \n",
      "\n",
      "   hemo   pcv        rc  htn   dm  cad  appet   pe  ane  \n",
      "0  15.4  44.0  5.200000  1.0  1.0  0.0    1.0  0.0  0.0  \n",
      "1  11.3  38.0  4.707435  0.0  0.0  0.0    1.0  0.0  0.0  \n",
      "2   9.6  31.0  4.707435  0.0  1.0  0.0    0.0  0.0  1.0  \n",
      "3  11.2  32.0  3.900000  1.0  0.0  0.0    0.0  1.0  1.0  \n",
      "4  11.6  35.0  4.600000  0.0  0.0  0.0    1.0  0.0  0.0  \n",
      "\n",
      "==================================================\n",
      "\n",
      "### 5. PCA Results and Commentary ###\n",
      "Original number of features (from Step 4): 20\n",
      "Number of PCA components to explain 95% variance: 2\n",
      "Total variance explained by these components: 96.18%\n",
      "\n",
      "Head of final PCA-transformed data:\n",
      "          PC1        PC2  classification\n",
      "0  -29.995106 -17.291876             1.0\n",
      "1   -6.617038 -39.852771             1.0\n",
      "2  271.283476 -45.755039             1.0\n",
      "3  -30.168461   5.187334             1.0\n",
      "4  -45.927400 -23.940960             1.0\n",
      "\n",
      "==================================================\n",
      "\n",
      "✅ Successfully processed data and exported to 'kidney_disease_processed_pca.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3045240/2161752541.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mean(), inplace=True)\n",
      "/tmp/ipykernel_3045240/2161752541.py:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mode()[0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, PolynomialFeatures, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "file_path = \"kidney_disease.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "df = df.replace(['\\t?', '\\t', '?', ''], np.nan)\n",
    "if 'id' in df.columns:\n",
    "    df = df.drop('id', axis=1)\n",
    "\n",
    "# --- 1. Feature Type Conversion ---\n",
    "# Convert columns that should be numeric\n",
    "numeric_cols = ['age', 'bp', 'sg', 'al', 'su', 'bgr', 'bu', 'sc', 'sod', 'pot', 'hemo', 'pcv', 'wc', 'rc']\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Convert binary/ordinal categorical columns\n",
    "binary_map_yes_no = {'yes': 1, 'no': 0}\n",
    "df['htn'] = df['htn'].map(binary_map_yes_no)\n",
    "df['dm'] = df['dm'].map(binary_map_yes_no)\n",
    "df['cad'] = df['cad'].map(binary_map_yes_no)\n",
    "df['pe'] = df['pe'].map(binary_map_yes_no)\n",
    "df['ane'] = df['ane'].map(binary_map_yes_no)\n",
    "df['appet'] = df['appet'].map({'good': 1, 'poor': 0})\n",
    "df['pcc'] = df['pcc'].map({'present': 1, 'notpresent': 0})\n",
    "df['ba'] = df['ba'].map({'present': 1, 'notpresent': 0})\n",
    "\n",
    "# Convert target variable\n",
    "df['classification'] = df['classification'].map({'ckd': 1, 'notckd': 0})\n",
    "\n",
    "print(\"### 1. Data Types after Conversion ###\")\n",
    "print(df.info())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- 1.5. Imputation (Necessary for ML algorithms) ---\n",
    "# We must fill missing values before proceeding\n",
    "for col in df.select_dtypes(include=np.number).columns:\n",
    "    df[col].fillna(df[col].mean(), inplace=True)\n",
    "    \n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "print(\"### Data Head after Imputation (Mean/Mode) ###\")\n",
    "print(df.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- 2. Categorical Encoding (rbc, pc) ---\n",
    "# Note: pcc and ba were already handled in step 1\n",
    "encode_cols = ['rbc', 'pc']\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in encode_cols:\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    # le.classes_ shows what was encoded (e.g., 'abnormal' -> 0, 'normal' -> 1)\n",
    "    print(f\"Encoding for '{col}': {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "\n",
    "print(\"\\n### 2. Data Head after Label Encoding 'rbc' and 'pc' ###\")\n",
    "print(df.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- 3. Feature Interaction and Polynomial Features ---\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop('classification', axis=1)\n",
    "y = df['classification']\n",
    "\n",
    "# Standardize data *before* polynomial features (good practice)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Create Polynomial Features (degree 2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "poly_features = poly.fit_transform(X_scaled)\n",
    "\n",
    "# Get names for the new features\n",
    "poly_feature_names = poly.get_feature_names_out(X.columns)\n",
    "df_poly = pd.DataFrame(poly_features, columns=poly_feature_names)\n",
    "\n",
    "print(\"### 3. Polynomial Features (Degree 2) ###\")\n",
    "print(f\"Original features: {X_scaled.shape[1]}\")\n",
    "print(f\"New total features (with interactions/polynomials): {df_poly.shape[1]}\")\n",
    "print(\"First 5 rows and 5 columns of the new polynomial features:\")\n",
    "print(df_poly.iloc[:, :5].head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- 4. Select Important Features ---\n",
    "# We use SelectKBest with the ANOVA F-test (f_classif)\n",
    "# Let's select the top 20 features\n",
    "k_best = 20\n",
    "selector = SelectKBest(score_func=f_classif, k=k_best)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_mask = selector.get_support()\n",
    "selected_features = X.columns[selected_mask]\n",
    "df_selected = pd.DataFrame(X_selected, columns=selected_features)\n",
    "\n",
    "print(f\"### 4. Selected Top {k_best} Features ###\")\n",
    "print(\"Selected feature names:\")\n",
    "print(list(selected_features))\n",
    "print(\"\\nHead of selected feature data:\")\n",
    "print(df_selected.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- 5. Reduce Dimensionality using PCA ---\n",
    "# Note: The data (df_selected) is already scaled from the polynomial step.\n",
    "# We will run PCA to capture 95% of the variance.\n",
    "pca = PCA(n_components=0.95)\n",
    "X_pca = pca.fit_transform(df_selected)\n",
    "\n",
    "# Create a new DataFrame for the PCA components\n",
    "pca_cols = [f'PC{i+1}' for i in range(X_pca.shape[1])]\n",
    "df_pca = pd.DataFrame(X_pca, columns=pca_cols)\n",
    "\n",
    "# Add the target variable back for the final file\n",
    "df_pca['classification'] = y\n",
    "\n",
    "print(\"### 5. PCA Results and Commentary ###\")\n",
    "print(f\"Original number of features (from Step 4): {df_selected.shape[1]}\")\n",
    "print(f\"Number of PCA components to explain 95% variance: {pca.n_components_}\")\n",
    "print(f\"Total variance explained by these components: {pca.explained_variance_ratio_.sum() * 100:.2f}%\")\n",
    "print(\"\\nHead of final PCA-transformed data:\")\n",
    "print(df_pca.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- Final Export ---\n",
    "output_filename = 'kidney_disease_processed_pca.csv'\n",
    "df_pca.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"✅ Successfully processed data and exported to '{output_filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2259b25-d0f7-4f22-a4cd-a23e5cbc7bb6",
   "metadata": {},
   "source": [
    "# 6.3 Practical Exercises: Third set\n",
    "The relevant dataset is available via the following link: Click here to see and download the dataset\n",
    "1. Show the correlation among all the features in the dataset and explain the results.\n",
    "2. Show the descriptive statistics for all the features.\n",
    "3. Check all the numeric features to see whether they are normally distributed or not.\n",
    "4. Draw box plots for all the numeric features and explain the results.\n",
    "5. Draw count plots for all the categorical features and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1305f-e445-47cd-8b71-d4287f86b794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
